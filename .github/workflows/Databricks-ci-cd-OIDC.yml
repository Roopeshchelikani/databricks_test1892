name: Deploy to Databricks with OIDC

on:
  push:
    branches:
      - main
  workflow_dispatch: # Allows manual triggering for testing

jobs:
  deploy-to-databricks:
    runs-on: ubuntu-latest
    permissions:
      id-token: write      # Required for OIDC to Azure
      contents: read       # Required to checkout the repo

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4 # Use latest checkout action

      - name: Login to Azure using OIDC 
        uses: azure/login@v1
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Get AAD access token for Databricks
        id: get-databricks-token
        run: |
          echo "Fetching token for resource: ${{ secrets.DATABRICKS_HOST_NAME }}" 
          databricks_token=$(az account get-access-token --resource ${{ secrets.DATABRICKS_HOST_NAME }} --query accessToken -o tsv)
          echo "token_value=${databricks_token}" >> $GITHUB_OUTPUT
          echo "::add-mask::${databricks_token}" # Mask the token in logs

      - name: Set up Databricks CLI
        uses: databricks/setup-cli@v0.218.0 # Installs the Databricks CLI

      - name: Deploy to Databricks Workspace
        env: # Environment variables are used by the Databricks CLI for authentication
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ steps.get-databricks-token.outputs.token_value }}
        run: |
          TARGET_PATH="/deployment/dev_OIDC_deploy" # Define your desired target path in Databricks
          SOURCE_PATH="./databricks" # Define the local source directory in your repository

          echo "Deploying notebooks from ${SOURCE_PATH} to ${TARGET_PATH} on ${DATABRICKS_HOST}"
          
          # 1. Ensure the parent directory exists (e.g., /deployment).
          # This prevents errors if /deployment itself doesn't exist.
          echo "Ensuring parent directory for ${TARGET_PATH} exists..."
          PARENT_DIR=$(dirname "${TARGET_PATH}") 
          databricks workspace mkdirs "${PARENT_DIR}" || true # Create parent dir if it doesn't exist. '|| true' prevents step failure.

          # 2. Attempt to delete the target directory if it exists.
          # The '|| true' ensures the step doesn't fail if the directory is not found on first run.
          echo "Attempting to delete existing directory ${TARGET_PATH} if it exists..."
          databricks workspace delete "${TARGET_PATH}" --recursive || true
          
          # 3. Import the directory with the corrected command name (import-dir)
          echo "Importing notebooks from ${SOURCE_PATH} to ${TARGET_PATH}..."
          databricks workspace import-dir "${SOURCE_PATH}" "${TARGET_PATH}"
          
          echo "Deployment attempt finished."
